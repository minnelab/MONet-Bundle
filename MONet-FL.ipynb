{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40543901dbd88c08",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# MONet-FL Tutorial \n",
    "\n",
    "## Federated Learning with MONet Bundle and NVIDIA Flare\n",
    "\n",
    "This tutorial will guide you through the process of running a nnUNet experiment in a Federated Learning context, by using the [MONet Bundle](https://github.com/SimoneBendazzoli93/MONet-Bundle) and the NVIDIA Flare API. The goal is to train a model on a dataset that is distributed across multiple sites, while ensuring data privacy and security.\n",
    "\n",
    "In this tutorial, we will use the nnUNet framework to train a model on a dataset that is distributed across multiple sites. The training will be done in a secure and privacy-preserving manner, by using the Federated Learning capabilities of the NVIDIA Flare API.\n",
    "\n",
    "To showcase all the functionalities of the MONet Bundle in Federated Learning, we will use the Decathlon Spleen dataset and consider the Spleen segmentation as our task of reference. \n",
    "\n",
    "The Spleen dataset (Task09_Spleen) was obtained from the Medical Segmentation Decathlon challenge [(Simpson et al., 2019)](https://arxiv.org/abs/1902.09063).\n",
    "\n",
    "\n",
    "### Model Training\n",
    "\n",
    "In detail, the following steps will be performed:\n",
    "\n",
    "0. Dataset Preparation: The dataset in the different sites will be prepared for training, harmonizing the data and creating the necessary files for training according to the nnUNet framework.\n",
    "1. nnUNet Experiment Planning and Preprocessing: One of the sites will be selected as the main site, where the nnUNet experiment will be planned and the data will be preprocessed. This step has to be done only on one site, as the nnUNet plans will be shared with the other sites.\n",
    "2. nnUNet Preprocessing: The data will be preprocessed according to the nnUNet plan in all the other sites.\n",
    "3. nnUNet Training: The model will be trained on the data of all the sites, using the nnUNet framework and aggregating the local gradients from the different sites.\n",
    "\n",
    "![](./images/Workflow.png)\n",
    "\n",
    "### Single-Site Training\n",
    "Single-Site training is also made available for the clients, to test the training process and evaluate the model at each individual site, establishing a baseline before moving on to the Federated Learning phase.\n",
    "In this fase, Data Preparation, Experiment Planning and Preprocessing and Training will be done on each site separately.\n",
    "\n",
    "### Cross-Site Validation\n",
    "After the training is completed, a cross-site validation will be performed to evaluate the model's performance across different sites. This step will ensure that the model is robust and generalizes well to data from different sites. The validation will be done by using a trained model from one site and evaluating it on the validation data from the other sites. In this step, we perform the inference from a trained model with MONAI Deploy, and then compute the metrics to evaluate the model's performance on external data.\n",
    "\n",
    "### Model Deployment\n",
    "In addition, we will perform some specific steps to prepare the model for deployment:\n",
    "- Convert the trained model to a TorchScript format, supported by MONAI Deploy.\n",
    "- Package the model into a MONAI bundle.\n",
    "- Upload the trained model to MLFlow.\n",
    "\n",
    "### PREREQUISITES\n",
    "This tutorial assumes that you have already installed the NVIDIA Flare API and have access to a Federated Learning cluster, with ``Lead`` role.\n",
    "\n",
    "Additionally, all the sites should have the necessary data for training ready in a known folder location (not necessarily the same location across all sites).\n",
    "\n",
    "To install the NVIDIA Flare API and the required Pytorch version, you can use the following command:\n",
    "\n",
    "```bash\n",
    "pip install nvflare==2.4.0rc6 light-the-torch\n",
    "ltt install torch==2.6.0\n",
    "pip install cryptography==42 # Required version for NVFlare\n",
    "pip install \"monai[all]\"\n",
    "pip install fire monai-nvflare==0.2.4 odict pyhocon\n",
    "\n",
    "pip install --no-deps git+https://github.com/SimoneBendazzoli93/MONAI.git@dev\n",
    "pip install git+https://github.com/SimoneBendazzoli93/nnUNet.git\n",
    "pip install git+https://github.com/SimoneBendazzoli93/monai-deploy-app-sdk.git@nifti-support\n",
    "pip install highdicom\n",
    "\n",
    "pip install batchgenerators==0.25 # Required version for nnUNet\n",
    "```\n",
    "\n",
    "The tutorial is designed to be executed within the [MAIA Platform](https://maia.app.cloud.cbh.kth.se). If you are running it somewhere else, you will need to adapt the paths accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95063771",
   "metadata": {},
   "source": [
    "## Configure the Federation in POC Mode\n",
    "\n",
    "Before starting the tutorial, we need to configure the federation in NVFlare POC (Proof of Concept) mode. This mode allows the federation to be configured within a single site, which is useful for testing and development purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PATH=$PATH:$HOME/.local/bin\n",
    "\n",
    "#export NVFLARE_POC_WORKSPACE=$(pwd)/\"NVFlare_POC\"\n",
    "export NVFLARE_POC_WORKSPACE=\"/home/maia-user/Data/NVFlare_POC\"\n",
    "nvflare poc prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897be93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "#export NVFLARE_POC_WORKSPACE=$(pwd)/\"NVFlare_POC\"\n",
    "export NVFLARE_POC_WORKSPACE=\"/home/maia-user/Data/NVFlare_POC\"\n",
    "\n",
    "nvflare poc start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dce1ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"NVFLARE_POC_WORKSPACE\"]=\"/home/maia-user/Data/NVFlare_POC\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f661ff-8a1d-4ab4-94d3-4d51858caebb",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dde56c-7b66-4a47-9b18-33c84b6561da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvflare.tool.poc.poc_commands import _prepare_poc, _start_poc, _stop_poc, _clean_poc\n",
    "\n",
    "_prepare_poc(\n",
    "    [\"site-1\",\"site-2\"],\n",
    "    2,\n",
    "    \"NVFlare_POC\",\n",
    ")\n",
    "_start_poc(\"NVFlare_POC\",[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e085b",
   "metadata": {},
   "source": [
    "## Split the Data Across Sites\n",
    "\n",
    "After starting the federation in POC mode, we need to split the data across the two different sites. In this tutorial, we will randomly split the Spleen Decathlon Spleen dataset into two parts, one for each site.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffcbb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subfiles(folder_path, prefix=None, suffix=None, join=True):\n",
    "    import os\n",
    "    if prefix is None:\n",
    "        prefix = \"\"\n",
    "    if suffix is None:\n",
    "        suffix = \"\"\n",
    "    files = []\n",
    "    for root, dirs, file in os.walk(folder_path):\n",
    "        for f in file:\n",
    "            if f.startswith(prefix) and f.endswith(suffix):\n",
    "                if join:\n",
    "                    files.append(os.path.join(root, f))\n",
    "                else:\n",
    "                    files.append(f)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545287c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from monai.apps import DecathlonDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f91b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MONAI_DATA_DIRECTORY\"] = \"/home/maia-user/Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac93935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "if directory is not None:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf01dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "DecathlonDataset(root_dir=root_dir, task=\"Task09_Spleen\", section=\"training\", download=True, cache_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc61419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "image_dir = os.environ[\"MONAI_DATA_DIRECTORY\"] + \"/Task09_Spleen/imagesTr\"\n",
    "label_dir = os.environ[\"MONAI_DATA_DIRECTORY\"] + \"/Task09_Spleen/labelsTr\"\n",
    "\n",
    "# Site 1 will have the \"Decathlon\" dataset format\n",
    "Path(os.environ[\"NVFLARE_POC_WORKSPACE\"]).joinpath(\"data/site-1/imagesTr\").mkdir(parents=True, exist_ok=True)\n",
    "Path(os.environ[\"NVFLARE_POC_WORKSPACE\"]).joinpath(\"data/site-1/labelsTr\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Site 2 will have the \"Subfolders\" dataset format\n",
    "Path(os.environ[\"NVFLARE_POC_WORKSPACE\"]).joinpath(\"data/site-2\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "images = subfiles(image_dir, prefix=\"spleen\", suffix=\".nii.gz\")\n",
    "labels = subfiles(label_dir, prefix=\"spleen\", suffix=\".nii.gz\")\n",
    "\n",
    "print(f\"Images: {len(images)}\")\n",
    "print(f\"Labels: {len(labels)}\")\n",
    "\n",
    "shuffle(images)\n",
    "\n",
    "for image in images[:len(images)//2]:\n",
    "    shutil.copy(image, Path(os.environ[\"NVFLARE_POC_WORKSPACE\"]).joinpath(\"data/site-1/imagesTr\"))\n",
    "    shutil.copy(image.replace(\"imagesTr\", \"labelsTr\"), Path(os.environ[\"NVFLARE_POC_WORKSPACE\"]).joinpath(\"data/site-1/labelsTr\"))\n",
    "\n",
    "for image in images[len(images)//2:]:\n",
    "    id = image.split(\"/\")[-1].split(\".\")[0]\n",
    "    Path(os.environ[\"NVFLARE_POC_WORKSPACE\"]).joinpath(f\"data/site-2/{id}\").mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy(image, Path(os.environ[\"NVFLARE_POC_WORKSPACE\"]).joinpath(f\"data/site-2/{id}/{id}_CT.nii.gz\"))\n",
    "    shutil.copy(image.replace(\"imagesTr\", \"labelsTr\"), Path(os.environ[\"NVFLARE_POC_WORKSPACE\"]).joinpath(f\"data/site-2/{id}/{id}_label.nii.gz\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac4857a",
   "metadata": {},
   "source": [
    "## Download MONet Bundle\n",
    "\n",
    "Next, we download the MONet Bundle, one per client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bad98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p ${NVFLARE_POC_WORKSPACE}/MONet-Bundles/site-1\n",
    "mkdir -p ${NVFLARE_POC_WORKSPACE}/MONet-Bundles/site-2\n",
    "\n",
    "wget  https://raw.githubusercontent.com/minnelab/MONet-Bundle/main/MONetBundle.zip -O ${NVFLARE_POC_WORKSPACE}/MONet-Bundles/site-1/MONetBundle.zip\n",
    "unzip -o ${NVFLARE_POC_WORKSPACE}/MONet-Bundles/site-1/MONetBundle.zip -d ${NVFLARE_POC_WORKSPACE}/MONet-Bundles/site-1\n",
    "rm ${NVFLARE_POC_WORKSPACE}/MONet-Bundles/site-1/MONetBundle.zip\n",
    "\n",
    "wget  https://raw.githubusercontent.com/minnelab/MONet-Bundle/main/MONetBundle.zip -O ${NVFLARE_POC_WORKSPACE}/MONet-Bundles/site-2/MONetBundle.zip\n",
    "unzip -o ${NVFLARE_POC_WORKSPACE}/MONet-Bundles/site-2/MONetBundle.zip -d ${NVFLARE_POC_WORKSPACE}/MONet-Bundles/site-2\n",
    "rm ${NVFLARE_POC_WORKSPACE}/MONet-Bundles/site-2/MONetBundle.zip\n",
    "\n",
    "cp -r ${NVFLARE_POC_WORKSPACE}/MONet-Bundles/site-2/MONetBundle/src /home/maia-user/shared/src/Spleen/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d788842381a2a684",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## FL Cluster Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de790771-2f3b-4a19-9b2d-be689445cdc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T18:19:11.262605Z",
     "start_time": "2024-09-30T18:19:07.990219Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nvflare.fuel.flare_api.flare_api import new_secure_session\n",
    "sess = new_secure_session(\n",
    "    \"admin@nvidia.com\",\n",
    "    \"/home/maia-user/Data/NVFlare_POC/example_project/prod_00/admin@nvidia.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa80a4-d5c2-49c7-8054-8c7392bf8fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sess.get_system_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1b313706f2a9f3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## List Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997fc41a-3720-4656-b8a8-d0a66dd02d61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jobs = sess.list_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ec84f-b654-4b5c-8fcf-6ae8704c95b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403328c4-4c08-4566-a748-48cb514c934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = jobs[-1]['job_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a852f47f73063e",
   "metadata": {},
   "source": [
    "## Terminate Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c5b9a823090c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.abort_job(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdce8b8961132613",
   "metadata": {},
   "source": [
    "## Job Preparation\n",
    "\n",
    "The first step in the Federated Learning process is to prepare the job configuration files. The job configuration files contain the necessary information to run the job on the Federated Learning cluster, such as the job type, the resources required, and the parameters for the job execution.\n",
    "\n",
    "The job configurations files are automatically generated by the script `nvflare_generate_job_configs`, which is installed together with this package. The script takes as input client-specific configuration, together with the experiment-specific configuration, and generates the job configuration files for each client.\n",
    "\n",
    "\n",
    "### Client Configuration\n",
    "The client-specific configuration file should be in the following format:\n",
    "\n",
    "\n",
    "```yaml\n",
    "data_dir: \"<DATASET_FOLDER>\"\n",
    "modality_dict:\n",
    "  ct: \"<CT_SUFFIX>\"\n",
    "  label: \"<SEG_MASK_SUFFIX>\"\n",
    "dataset_format: \"<DATASET_FORMAT>\"\n",
    "patient_id_in_file_identifier: True\n",
    "nnunet_root_folder: \"<NNUNET_ROOT_FOLDER>\"\n",
    "client_name: \"<CLIENT_NAME>\"\n",
    "subfolder_suffix: \"<SUBFOLDER_SUFFIX>\" [OPTIONAL]\n",
    "bundle_root: \"<BUNDLE_ROOT>\" [OPTIONAL]\n",
    "# Optional parameters for MONAI Deploy Inference, we discuss them in the Model Deployment section\n",
    "app_path: \"\"\n",
    "app_model_path: \"\"\n",
    "app_output_path: \"\"\n",
    "model_name: \"\"\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "`dataset_format` should refer to one of  these three different formats, according to the `data_dir` structure:\n",
    "1. `subfolders`: The dataset is organized in subfolders, where each subfolder corresponds to a subject and contains the images and labels for that subject.\n",
    "```plaintext\n",
    "  [Dataset_folder]\n",
    "        [Subject_0]\n",
    "            - Subject_0_image0.nii.gz    # Subject_0 modality 0\n",
    "            - Subject_0_image1.nii.gz    # Subject_0 modality 1\n",
    "            - Subject_0_mask.nii.gz      # Subject_0 semantic segmentation mask\n",
    "        [Subject_1]\n",
    "            - Subject_1_image0.nii.gz    # Subject_1 modality 0\n",
    "            - Subject_1_image1.nii.gz    # Subject_1 modality 1\n",
    "            - Subject_1_mask.nii.gz      # Subject_1 semantic segmentation mask\n",
    "        ...\n",
    "\n",
    "```\n",
    "2. `decathlon`: The dataset is organized in the format of the Medical Decathlon challenge, where the images and labels are stored in separate folders.\n",
    "```plaintext\n",
    "  [Dataset_folder]\n",
    "        [imagesTr]\n",
    "            - Subject_0_image0.nii.gz    # Subject_0 modality 0\n",
    "            - Subject_0_image1.nii.gz    # Subject_0 modality 1\n",
    "            - Subject_1_image0.nii.gz    # Subject_1 modality 0\n",
    "            - Subject_1_image1.nii.gz    # Subject_1 modality 1\n",
    "        [labelsTr]\n",
    "            - Subject_1_mask.nii.gz      # Subject_1 semantic segmentation mask\n",
    "            - Subject_0_mask.nii.gz      # Subject_0 semantic segmentation mask\n",
    "        ...\n",
    "\n",
    "```\n",
    "\n",
    "3. `nnunet`: The dataset has been already prepared according to the nnUNet framework, with the images and labels stored in separate folders.\n",
    "```plaintext\n",
    "  [nnUNet_raw]\n",
    "      [DatasetXYZ_TaskName]  # THIS IS THE DATASET FOLDER\n",
    "          dataset.json\n",
    "          [imagesTr]\n",
    "              - Subject_0_image0.nii.gz    # Subject_0 modality 0\n",
    "              - Subject_0_image1.nii.gz    # Subject_0 modality 1\n",
    "              - Subject_1_image0.nii.gz    # Subject_1 modality 0\n",
    "              - Subject_1_image1.nii.gz    # Subject_1 modality 1\n",
    "          [labelsTr]\n",
    "              - Subject_1_mask.nii.gz      # Subject_1 semantic segmentation mask\n",
    "              - Subject_0_mask.nii.gz      # Subject_0 semantic segmentation mask\n",
    "        ...\n",
    "\n",
    "```\n",
    "\n",
    "`nnunet_root_folder` should refer to the root folder used by the nnUnet framework, where the nnUNet experiments are stored.\n",
    "For the `subfolders` and `decathlon` dataset formats, this folder is created during the dataset preparation step. \n",
    "For the `nnunet` dataset format, this folder should contain the nnUNet experiments, with the following structure:\n",
    "```plaintext\n",
    "  [nnunet_root_folder]\n",
    "      [nnUNet_raw_data_base]\n",
    "          [DatasetXYZ_TaskName]\n",
    "              dataset.json\n",
    "              [imagesTr]\n",
    "                  - Subject_0_image0.nii.gz    # Subject_0 modality 0\n",
    "                  - Subject_0_image1.nii.gz    # Subject_0 modality 1\n",
    "                  - Subject_1_image0.nii.gz    # Subject_1 modality 0\n",
    "                  - Subject_1_image1.nii.gz    # Subject_1 modality 1\n",
    "              [labelsTr]\n",
    "                  - Subject_1_mask.nii.gz      # Subject_1 semantic segmentation mask\n",
    "                  - Subject_0_mask.nii.gz      # Subject_0 semantic segmentation mask\n",
    "            ...\n",
    "\n",
    "```\n",
    "\n",
    "`modality_dict` is a dictionary that maps the modality names to the file suffixes. The suffixes are used to identify the files that correspond to the different modalities in the dataset. For example, if the CT images have the suffix `_CT.nii.gz`, the entry in the `modality_dict` should be `ct: \"_CT.nii.gz\"`.\n",
    "\n",
    "\n",
    "`patient_id_in_file_identifier` is a flag used to specify if the patient ID is included in the file name. If this flag is set to `True`, the patient ID will be extracted from the file name. If this flag is set to `False`, the patient ID will be extracted from the file path. If set to `False`, the filename should only contain the modality suffix.\n",
    "\n",
    "`client_name` is a unique identifier for the client.\n",
    "\n",
    "`subfolder_suffix` is an optional parameter that specifies the suffix of the subfolders that contain the images and labels for each subject. This parameter is used when the dataset is organized in subfolders, and the subfolders have a specific suffix that needs to be removed to extract the patient ID.\n",
    "\n",
    "`bundle_root` is an optional parameter that specifies the root folder where the MONet Bundle is stored.\n",
    "\n",
    "`app_path` is an optional parameter that specifies the path to the MONAI Deploy application to be used for inference. This parameter is used when the model is deployed using MONAI Deploy.\n",
    "\n",
    "`app_model_path` is an optional parameter that specifies the path to the TorchScript model file to be used by the MONAI Deploy application. This parameter is used when the model is deployed using MONAI Deploy.\n",
    "\n",
    "`app_output_path` is an optional parameter that specifies the path where the output of the MONAI Deploy Inference will be stored. This parameter is used when the model is deployed using MONAI Deploy.\n",
    "\n",
    "`model_name` is an optional parameter that specifies the name of the model to be used by the MONAI Deploy application. This parameter is used when the model is deployed using MONAI Deploy.\n",
    "\n",
    "### Experiment Configuration\n",
    "\n",
    "The experiment-specific configuration file should be in the following format:\n",
    "\n",
    "```yaml\n",
    "dataset_name_or_id: \"<DATASET_NAME_OR_ID>\"\n",
    "experiment_name: \"<EXPERIMENT_NAME>\"\n",
    "tracking_uri: \"<TRACKING_URI>\"\n",
    "mlflow_token: \"<MLFLOW_TOKEN>\" [OPTIONAL]\n",
    "nnunet_trainer: \"<NNUNET_TRAINER>\" [OPTIONAL]\n",
    "num_rounds: \"<NUM_ROUNDS>\"\n",
    "start_round: \"<START_ROUND>\"\n",
    "local_epochs: \"<LOCAL_EPOCHS>\"\n",
    "server_bundle_root: \"<SERVER_BUNDLE_ROOT>\" [OPTIONAL]\n",
    "modality_list:\n",
    "  - \"<MODALITY_1>\"\n",
    "  - \"<MODALITY_2>\"\n",
    "label_dict:\n",
    "  class1: 1\n",
    "# Extra parameters for the MONet Bundle\n",
    "#bundle_extra_config:\n",
    "#  resume_epoch: \"latest\"  # Optional, used to resume training from a specific epoch\n",
    "#  region_based: True # Optional, used to enable region-based training\n",
    "#  is_federated: True # Optional, used to enable federated training when preparing the bundle. Set to False if you want to prepare the bundle for single-site training\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "`dataset_name_or_id` and `experiment_ame` are used as a reference to the nnUNet Dataset ID and Experiment Name, respectively. These values are used to identify the nnUNet experiment in the nnUNet framework. `Experiment Name` is also used to identify the experiment in the MLFlow server, and to generate the zipped nnUNet model file (as `<Experiment Name>.zip`).\n",
    "\n",
    "\n",
    "`mlflow_token` and `tracking_uri` are used to connect to the MLFlow server, where the experiments are logged, and the trained models are uploaded.\n",
    "\n",
    "`nnunet_trainer` is an optional parameter that specifies the nnUNet trainer to be used for training. If this parameter is not specified, the default nnUNet trainer will be used.\n",
    "\n",
    "`num_rounds` is the number of rounds to be executed in the Federated Learning process. This parameter is used to control the number of rounds of training that will be performed on the Federated Learning cluster.\n",
    "\n",
    "`start_round` is the round number from which to start the training. This parameter is used to control the starting point of the training process.\n",
    "\n",
    "`local_epochs` is the number of local epochs to be executed on each client. This parameter is used to control the number of epochs that will run on each client before the local model is sent to the server for aggregation.\n",
    "\n",
    "`server_bundle_root` is the root folder where the MONAI Bundle is stored. This parameter is used to specify the location of the MONAI Bundle that will be used for model deployment.\n",
    "\n",
    "`modality_list` is a list of the modalities that will be used for training. This parameter is used to specify the modalities that will be used in the nnUNet experiment.\n",
    "\n",
    "\n",
    "`label_dict` is a dictionary that maps the class names to the class IDs. The class IDs are used to identify the different classes in the dataset. For example, if the dataset has two classes, `Class1` and `Class2`, with IDs 1 and 2, respectively, the entry in the `label_dict` should be `label_dict: Class1: 1, Class2: 2`.\n",
    "\n",
    "### Prepare the Job Configuration Files\n",
    "\n",
    "To prepare the job configuration files, run the following command:\n",
    "\n",
    "```python\n",
    "monai.nvflare.nvflare_generate_job_configs.generate_configs(client_files, experiment_file, script_dir, job_dir)\n",
    "```\n",
    "where:\n",
    "- `<CLIENT_CONFIG_FILE_1>`, `<CLIENT_CONFIG_FILE_2>`, ... are the client-specific configuration files for each client.\n",
    "- `<experiment_file>` is the experiment-specific configuration file.\n",
    "- `<script_dir>` is the directory where the job scripts and python files are stored.\n",
    "- `<job_dir>` is the directory where the job configuration files will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af82cf7-a0b9-4cf7-ae47-03a7a1d43927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    " \n",
    "ROOT_FOLDER = \"/home/maia-user/shared\"\n",
    "sys.path.append(ROOT_FOLDER)\n",
    "\n",
    "Path(ROOT_FOLDER).joinpath(\"Experiments\").mkdir(parents=True, exist_ok=True)\n",
    "Path(ROOT_FOLDER).joinpath(\"Clients\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1464aecf-f69b-4cd5-b4ab-82e7b748ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/maia-user/shared/Experiments/Spleen.yaml\n",
    "dataset_name_or_id: \n",
    "    site-1: \n",
    "        id: \"009\"\n",
    "        name: \"Task09_Spleen-1\"\n",
    "    site-2:\n",
    "        id: \"010\"\n",
    "        name: \"Task09_Spleen-2\"\n",
    "experiment_name: \"Task09_Spleen\"\n",
    "tracking_uri: \"http://localhost:5000\"\n",
    "nnunet_trainer: \"nnUNetTrainer_10epochs\"\n",
    "num_rounds: 10\n",
    "start_round: 0\n",
    "local_epochs: 1\n",
    "server_bundle_root: \"/workspace/FedSpleen_Bundle\"\n",
    "modality_list:\n",
    "- \"CT\"\n",
    "label_dict:\n",
    "    Spleen: 1\n",
    "bundle_extra_config:\n",
    "  is_federated: False\n",
    "  resume_epoch: '\"latest\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede053d8-280a-4541-8c2d-41c02c54878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/maia-user/shared/Clients/site-1.yaml\n",
    "data_dir: \"/home/maia-user/Data/NVFlare_POC/data/site-1\"\n",
    "modality_dict:\n",
    "  ct: \".nii.gz\"\n",
    "  label: \".nii.gz\"\n",
    "dataset_format: \"decathlon\"\n",
    "patient_id_in_file_identifier: True\n",
    "nnunet_root_folder: \"/home/maia-user/Data/NVFlare_POC/nnUNet\"\n",
    "client_name: \"site-1\"\n",
    "bundle_root: \"/home/maia-user/Data/NVFlare_POC/MONet-Bundles/site-1/MONetBundle\"\n",
    "app_path: \"\"\n",
    "app_model_path: \"\"\n",
    "app_output_path: \"\"\n",
    "model_name: \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe0bdc6-b7cb-4a0a-9b2f-27c30452e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/maia-user/shared/Clients/site-2.yaml\n",
    "data_dir: \"/home/maia-user/Data/NVFlare_POC/data/site-2\"\n",
    "modality_dict:\n",
    "  ct: \"_CT.nii.gz\"\n",
    "  label: \"_label.nii.gz\"\n",
    "dataset_format: \"subfolders\"\n",
    "patient_id_in_file_identifier: True\n",
    "nnunet_root_folder: \"/home/maia-user/Data/NVFlare_POC/nnUNet\"\n",
    "client_name: \"site-2\"\n",
    "bundle_root: \"/home/maia-user/Data/NVFlare_POC/MONet-Bundles/site-2/MONetBundle\"\n",
    "app_path: \"\"\n",
    "app_model_path: \"\"\n",
    "app_output_path: \"\"\n",
    "model_name: \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e31d40-9c32-4c44-925e-3542c866c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"Spleen\"\n",
    "clients = [\n",
    "    \"site-1\",\n",
    "    \"site-2\"\n",
    "]\n",
    "\n",
    "Path(ROOT_FOLDER).joinpath(\"src\").joinpath(experiment).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36832ffc-5057-4a35-bcc6-ac798b9cbe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm -r /home/maia-user/shared/Jobs/Spleen/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18cdd17-8786-4cca-a0f1-93776f7299b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from monai.nvflare.nvflare_generate_job_configs import generate_configs\n",
    "\n",
    "generate_configs(\n",
    "    [str(Path(ROOT_FOLDER).joinpath(\"Clients\",client+\".yaml\")) for client in clients],\n",
    "    str(Path(ROOT_FOLDER).joinpath(\"Experiments\",experiment+\".yaml\")),\n",
    "    str(Path(ROOT_FOLDER).joinpath(\"src\",experiment)),\n",
    "    str(Path(ROOT_FOLDER).joinpath(\"Jobs\",experiment)),\n",
    "    tasks = [\"prepare_bundle\",\"train\"]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f57247-c564-4703-bcc7-4e6f12e82e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_DIR=str(Path(ROOT_FOLDER).joinpath(\"Jobs\",experiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9545a4e6-3c6c-4e2c-99f8-3b5ce4b0239c",
   "metadata": {},
   "source": [
    "## 0.0 Check Python Packages\n",
    "\n",
    "This initial step is to check that the required Python packages are installed and available in the environment. This is done by importing the necessary packages and checking their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d504722-79a7-4968-9f3e-d837c91dd396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from monai.nvflare.nvflare_nnunet import run_job\n",
    "\n",
    "task_name = \"check_client_packages\"\n",
    "\n",
    "job_id = run_job(sess, task_name, str(Path(JOB_DIR).joinpath(\"jobs\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5da18f-0440-4266-8fff-7ac855fe2f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.monitor_job(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25774b07-99d3-4229-92fd-9bff414e21bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dir = sess.download_job_result(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4aa553-a00e-45b3-ae0c-08054ef59bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "with open(Path(job_dir).joinpath(\"workspace\",\"package_report\",\"package_report.json\"),\"r\") as f:\n",
    "    package_report = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d61cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(package_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e28d47fb25446e1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 0.1 Prepare Dataset\n",
    "\n",
    "In this step, the dataset in the different sites will be prepared for training, harmonizing the data structures and creating the necessary files for training according to the nnUNet framework.\n",
    "\n",
    "This step is internally calling the `nnUNetV2Runner` from MONAI, performing the `runner.convert_dataset()`.\n",
    "\n",
    "\n",
    "Before running this step, you can start a local MLFlow server, to log the task and the FL experiments:\n",
    "```bash\n",
    "mkdir MLFlow\n",
    "cd MLFlow\n",
    "mlflow server\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42596bfcf2aea4c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Submit the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775e1b39443362d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from monai.nvflare.nvflare_nnunet import run_job\n",
    "\n",
    "task_name = \"prepare\"\n",
    "\n",
    "job_id = run_job(sess, task_name, str(Path(JOB_DIR).joinpath(\"jobs\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d4bcf9051775b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sess.monitor_job(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd96da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = \"site-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98df7413cdbf4afd",
   "metadata": {},
   "source": [
    "To monitor the job, or print the logs from either the server or the client side, you can use the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f849b-e09f-4cda-bf01-ee36db5cd4ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(sess.api.do_command(f\"cat server log.txt\")['data'][0]['data'])\n",
    "print(sess.api.do_command(f\"cat {client_id} {job_id}/log.txt\")['data'][0]['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a900685dc5fc0604",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Download the Job Results\n",
    "When the job is completed, you can download the results, which, for the Prepare Dataset job, will contain the dataset.json file, containing the information about the dataset. In detail, for each client, the dataset.json file will list the dataset files and wheter all the files are valid or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830fded9516c0d6a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "job_dir = sess.download_job_result(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77cb85fa5dc8c82",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "with open(Path(job_dir).joinpath(\"workspace\",\"prepare\",\"data_dict.json\"),\"r\") as f:\n",
    "    dataset_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb243c580fb4c54d",
   "metadata": {},
   "source": [
    "To inspect the dataset, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac86ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = \"site-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc9091f580cdb56",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "verified = True\n",
    "\n",
    "for case in dataset_dict[client_id][\"training\"]:\n",
    "    for key in case:\n",
    "        if key.endswith(\"_is_file\") and not case[key]:\n",
    "            file = case[key[:-len(\"_is_file\")]]\n",
    "            print(f\"Error: {file} is not a valid file!\")\n",
    "            verified = False\n",
    "if verified:\n",
    "    print(f\"Dataset succesfully verified for client {client_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23b6d829e777c3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(len(dataset_dict[client_id][\"training\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e0f76dbaa30073",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1. Plan and Preprocess\n",
    "\n",
    "After the dataset has been prepared, the nnUNet experiment has to be planned and the data preprocessed. This step has to be done only on one site, as the nnUNet plans will be shared with the other sites.\n",
    "\n",
    "The steps to plan and preprocess the nnUNet experiment are the following:\n",
    "1. Run the `plan_and_preprocess` job on the chosen site.\n",
    "2. Extract the nnUNet plans from the job results.\n",
    "3. Share the nnUNet plans with the other sites.\n",
    "4. Run the `preprocess` job on the other sites.\n",
    "\n",
    "\n",
    "This step is internally calling the `nnUNetV2Runner` from MONAI, performing the ` runner.plan_and_process()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aee7d58-d8f9-4bb7-99c6-9b4d4fd8641a",
   "metadata": {},
   "source": [
    "### Submit the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38972ad-b061-4963-8327-806d9ab378c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from monai.nvflare.nvflare_nnunet import run_job\n",
    "\n",
    "task_name = \"plan_and_preprocess\"\n",
    "\n",
    "job_id = run_job(sess, task_name, str(Path(JOB_DIR).joinpath(\"jobs\")),\n",
    "                 {\"site-1\":\"\"} # Select only site-1 for this task\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901df284-f6ac-41cc-b580-cd32ed2b9381",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.monitor_job(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649a99ae-271d-4ca9-85a3-0d64c59e5c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client_id = \"site-1\"\n",
    "#print(sess.api.do_command(f\"tail server log.txt\")['data'][0]['data'])\n",
    "print(sess.api.do_command(f\"cat {client_id} {job_id}/log.txt\")['data'][0]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302cfe9c-269e-4164-9e14-cb7a316a691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dir = sess.download_job_result(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff74355-fc9d-446b-8352-08cf3e9a0fd1",
   "metadata": {},
   "source": [
    "### Inspect nnUNetPlans.json\n",
    "\n",
    "The nnUNet plans are stored in the `nnUNetPlans.json` file, which contains the configuration for the nnUNet experiment. The file can be found in the `workspace/nnUNet_preprocessing` folder of the job results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e3868d-1f0e-4649-8031-79732ad6ad95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(Path(job_dir).joinpath(\"workspace\",\"nnUNet_preprocessing\",\"nnUNetPlans.json\"),\"r\") as f:\n",
    "    nnunet_plans = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e246e-c7bf-4861-a6cd-b3f35156aa24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(json.dumps(nnunet_plans[\"site-1\"],indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30195c6c-f9ba-41d6-a060-8518bb5f7994",
   "metadata": {},
   "source": [
    "### Copy nnUNetPlans into Transfer Folder\n",
    "\n",
    "To share the nnUNet plans with the other sites, copy the `nnUNetPlans.json` file into the `src/Spleen` Folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771b3750-e06e-4579-a6bc-daa90a57ca04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(Path(\"/home/maia-user/shared/src/Spleen/nnUNetPlans.json\"),\"w\") as f:\n",
    "    json.dump(nnunet_plans[\"site-1\"],f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77d2992",
   "metadata": {},
   "source": [
    "And regenerate the job configuration files, so that the other sites can use the nnUNet plans for preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f198f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_configs(\n",
    "    [str(Path(ROOT_FOLDER).joinpath(\"Clients\",client+\".yaml\")) for client in clients],\n",
    "    str(Path(ROOT_FOLDER).joinpath(\"Experiments\",experiment+\".yaml\")),\n",
    "    str(Path(ROOT_FOLDER).joinpath(\"src\",experiment)),\n",
    "    str(Path(ROOT_FOLDER).joinpath(\"Jobs\",experiment)),\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71ce9bbbdd895",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. Preprocess\n",
    "\n",
    "After the nnUNet plans have been shared with the other sites, the data has to be preprocessed according to the nnUNet plans. This step has to be done on all the sites, except the one where the nnUNet experiment has been planned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c02f9-0260-43c7-a52a-464b945c6ee6",
   "metadata": {},
   "source": [
    "### Submit Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d00082ded26cd86",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from monai.nvflare.nvflare_nnunet import run_job\n",
    "\n",
    "task_name = \"preprocess\"\n",
    "\n",
    "job_id = run_job(sess, task_name, str(Path(JOB_DIR).joinpath(\"jobs\")),\n",
    "                 {\"site-2\":\"\"} # Select only site-1 for this task\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd04d7cc-8986-4c8e-bfc8-c5839651125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.monitor_job(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83de4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dir = sess.download_job_result(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d957fb19-834d-4ab8-824a-64d7baa108ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(sess.api.do_command(f\"cat server log.txt\")['data'][0]['data'])\n",
    "print(sess.api.do_command(f\"cat {client_id} {job_id}/log.txt\")['data'][0]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad755917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(Path(job_dir).joinpath(\"workspace\",\"nnUNet_preprocessing\",\"nnUNetPlans.json\"),\"r\") as f:\n",
    "    nnunet_plans = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca80ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(nnunet_plans[\"site-2\"],indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97370ec-c6e4-40f1-badb-56ed04dc19da",
   "metadata": {},
   "source": [
    "## 3.0 Single-Site nnUNet Training\n",
    "\n",
    "Once the dataset has been prepared and preprocessed following the nnUNet plans, the nnUNet training is ready to begin. In this phase, we will train a simple nnUNet model on a single site using the MONet Bundle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25378518",
   "metadata": {},
   "source": [
    "First, prepare the MONet Bundle and verify its configuration for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3676428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from monai.nvflare.nvflare_nnunet import run_job\n",
    "\n",
    "task_name = \"prepare_bundle\"\n",
    "\n",
    "job_id = run_job(sess, task_name, str(Path(JOB_DIR).joinpath(\"jobs\")),\n",
    "                 {\"site-1\":\"\"} # Select only site-1 for this task\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f924f8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dir = sess.download_job_result(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b43b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "with open(Path(job_dir).joinpath(\"workspace\",\"nnUNet_prepare_bundle\",\"bundle_config.json\"),\"r\") as f:\n",
    "    bundle_config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b89cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(bundle_config[\"site-1\"], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5713fc5b",
   "metadata": {},
   "source": [
    "Then, you can start the training job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f07f91-32de-4a34-864d-cecc898a5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from monai.nvflare.nvflare_nnunet import run_job\n",
    "\n",
    "task_name = \"train\"\n",
    "\n",
    "job_id = run_job(sess, task_name, str(Path(JOB_DIR).joinpath(\"jobs\")),\n",
    "                 {\"site-1\":\"\"} # Select only site-1 for this task\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d76c1dc",
   "metadata": {},
   "source": [
    "You can monitor the training on MLFlow, or:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b8c5c5-6a67-4684-958e-79de02e028c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.monitor_job(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.abort_job(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d7aff9",
   "metadata": {},
   "source": [
    "Once the training is completed, the validation will be performed on the validation set, and the results will be logged in MLFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b89e29-8ab4-4fde-a46d-d0f6112cdf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dir = sess.download_job_result(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93278c9b-1736-4ac7-8cb2-45e201b4c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "with open(Path(job_dir).joinpath(\"workspace\",\"nnUNet_train\",\"val_summary.json\"),\"r\") as f:\n",
    "    print(json.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463011e88cf642ec",
   "metadata": {},
   "source": [
    "To check the training logs, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224b65a-af93-4982-87ce-fe63d8297f59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sess.api.do_command(f\"cat {client_id} {job_id}/log.txt\")['data'][0]['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d5ac62",
   "metadata": {},
   "source": [
    "### Convert the trained MONet Bundle to a TorchScript model\n",
    "\n",
    "This step is needed to run the Cross-Site Validation and the Model Deployment steps, as the MONAI Deploy Inference requires a TorchScript model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74c32db",
   "metadata": {},
   "source": [
    "IMPORTANT! The native `dynamic-network-architectures` package, used by nnUNet, is not compatible with the TorchScript format.\n",
    "\n",
    "Install this version:\n",
    "\n",
    "```bash\n",
    "pip install --force-reinstall --no-deps git+https://github.com/SimoneBendazzoli93/dynamic-network-architectures.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601dc2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python convert_ckpt_to_ts.py --bundle_root /home/maia-user/Data/NVFlare_POC/MONet-Bundles/site-1/MONetBundle \\\n",
    "                             --checkpoint_name checkpoint_epoch=10.pt \\\n",
    "                             --fold 0 \\\n",
    "                             --nnunet_trainer_name nnUNetTrainer_10epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4bdbde",
   "metadata": {},
   "source": [
    "## Cross-Site Evaluation\n",
    "\n",
    "To run the cross-site evaluation, we will use the trained model from one site and evaluate it on the validation data from the other sites. \n",
    "As a requirement, you need to need to have the MONAI Deploy package installed, including the `ai_spleen_nifti_nnunet_seg_app`, which is used to run the inference on the validation NIFTI data.\n",
    "\n",
    "Update the site-specific configuration files to include the MONAI Deploy application path and the TorchScript model path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed22176",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Clone the repo shallowly\n",
    "\n",
    "\n",
    "cd /home/maia-user/Data/NVFlare_POC/example_project/prod_00/site-2\n",
    "\n",
    "git clone --depth 1 --filter=blob:none --sparse --branch nifti-support https://github.com/SimoneBendazzoli93/monai-deploy-app-sdk.git\n",
    "cd monai-deploy-app-sdk\n",
    "\n",
    "# Enable sparse checkout and set the folder\n",
    "git sparse-checkout set examples/apps/ai_spleen_nifti_nnunet_seg_app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d5cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p /home/maia-user/Data/CrossSite_Validation/spleen_site-2/spleen_site-1_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943b053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/maia-user/shared/Clients/site-2.yaml\n",
    "data_dir: \"/home/maia-user/Data/NVFlare_POC/data/site-2\"\n",
    "modality_dict:\n",
    "  ct: \".nii.gz\"\n",
    "  label: \".nii.gz\"\n",
    "dataset_format: \"decathlon\"\n",
    "patient_id_in_file_identifier: True\n",
    "nnunet_root_folder: \"/home/maia-user/Data/NVFlare_POC/nnUNet\"\n",
    "client_name: \"site-2\"\n",
    "bundle_root: \"/home/maia-user/Data/NVFlare_POC/MONet-Bundles/site-2/MONetBundle\"\n",
    "app_path: \"monai-deploy-app-sdk/examples/apps/ai_spleen_nifti_nnunet_seg_app\"\n",
    "app_model_path: \"/home/maia-user/Data/NVFlare_POC/MONet-Bundles/site-1/MONetBundle/models/fold_0/model.ts\"\n",
    "app_output_path: \"/home/maia-user/Data/CrossSite_Validation/spleen_site-2/spleen_site-1_predictions\"\n",
    "model_name: \"spleen_site-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b5bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_configs(\n",
    "    [str(Path(ROOT_FOLDER).joinpath(\"Clients\",client+\".yaml\")) for client in clients],\n",
    "    str(Path(ROOT_FOLDER).joinpath(\"Experiments\",experiment+\".yaml\")),\n",
    "    str(Path(ROOT_FOLDER).joinpath(\"src\",experiment)),\n",
    "    str(Path(ROOT_FOLDER).joinpath(\"Jobs\",experiment)),\n",
    "    tasks=[\"cross_site_validation\"]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f438804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from monai.nvflare.nvflare_nnunet import run_job\n",
    "\n",
    "task_name = \"cross_site_validation\"\n",
    "\n",
    "job_id = run_job(sess, task_name, str(Path(JOB_DIR).joinpath(\"jobs\")),\n",
    "                 {\"site-2\":\"\"} # Select only site-1 for this task\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9bb767",
   "metadata": {},
   "source": [
    "## 3.1 Federated Learning Training\n",
    "\n",
    "The Federated Learning training will aggregate the local gradients from the different sites and update the global model. The Federated Learning training will be conducted in rounds, with each round consisting of a number of local epochs. The Federated Learning training will be conducted in a secure and privacy-preserving manner, with the data remaining on the client side and only the local gradients being shared with the server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a50b0c4",
   "metadata": {},
   "source": [
    "Prior to starting the FL training, follow the steps below to correctly configure the federation.\n",
    "\n",
    "\n",
    "On the Server Side:\n",
    "\n",
    "1. Install the required python packages and download the MONet Bundle to the server.\n",
    "\n",
    "2. Upload the `plans.json` and `dataset.json` files to the server (in `<BUNDLE_ROOT>/models`).\n",
    "`dataset.json` can be in the form:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"task\": \"Dataset109_Task09_Spleen\",\n",
    "    \"dim\": 3,\n",
    "    \"test_labels\": true,\n",
    "    \"tensorImageSize\": \"4D\",\n",
    "    \"channel_names\": {\"0\": \"ct\"},\n",
    "    \"labels\": {\"background\": 0, \"Spleen\": 1},\n",
    "    \"numTraining\": 0,\n",
    "    \"numTest\": 0,\n",
    "    \"training\": [],\n",
    "    \"test\": [],\n",
    "    \"file_ending\": \".nii.gz\"\n",
    "}\n",
    "```\n",
    "3. Specify `bundle_root` in the server train bundle configuration file (`<BUNDLE_ROOT>/configs/train.yaml`):\n",
    "\n",
    "```yaml\n",
    "network_def_fl:\n",
    "    _target_: $monai.apps.nnunet.nnunet_bundle.get_network_from_nnunet_plans\n",
    "    plans_file: \"$@bundle_root+'/models/plans.json'\"\n",
    "    dataset_file: \"$@bundle_root+'/models/dataset.json'\"\n",
    "    configuration: '@nnunet_configuration'\n",
    "```\n",
    "\n",
    "4. Optionally, change in the train bundle configuration file (`<BUNDLE_ROOT>/configs/train.yaml`) the `nnunet_trainer_class_name`, and `nnunet_plans_identifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da15c19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "export NVFLARE_POC_WORKSPACE=\"/home/maia-user/Data/NVFlare_POC\"\n",
    "\n",
    "mkdir -p ${NVFLARE_POC_WORKSPACE}/MONet-Bundles/server\n",
    "\n",
    "wget  https://raw.githubusercontent.com/minnelab/MONet-Bundle/main/MONetBundle.zip -O ${NVFLARE_POC_WORKSPACE}/MONet-Bundles/server/MONetBundle.zip\n",
    "unzip -o ${NVFLARE_POC_WORKSPACE}/MONet-Bundles/server/MONetBundle.zip -d ${NVFLARE_POC_WORKSPACE}/MONet-Bundles/server\n",
    "rm ${NVFLARE_POC_WORKSPACE}/MONet-Bundles/server/MONetBundle.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b94c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "export NVFLARE_POC_WORKSPACE=\"/home/maia-user/Data/NVFlare_POC\"\n",
    "\n",
    "cp -r ${NVFLARE_POC_WORKSPACE}/nnUNet/nnUNet_preprocessed/Dataset009_site-1/nnUNetPlans.json ${NVFLARE_POC_WORKSPACE}/MONet-Bundles/server/MONetBundle/models/plans.json\n",
    "cp -r ${NVFLARE_POC_WORKSPACE}/nnUNet/nnUNet_preprocessed/Dataset009_site-1/dataset.json ${NVFLARE_POC_WORKSPACE}/MONet-Bundles/server/MONetBundle/models/dataset.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff61fe7b",
   "metadata": {},
   "source": [
    "On the Client Side:\n",
    "\n",
    "1. Specify the following parameters in the `Experiment` configuration:\n",
    "   \n",
    "```yaml\n",
    "   num_rounds: 100\n",
    "   server_bundle_root: \"<SERVER_BUNDLE_ROOT>\"\n",
    "   start_round: 0\n",
    "   local_epochs: 10\n",
    "   bundle_extra_config:\n",
    "      is_federated: True\n",
    " ```\n",
    "\n",
    "\n",
    "2. Re-execute the `generate_job_configs` script to update the job configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf3b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/maia-user/shared/Experiments/Spleen.yaml\n",
    "dataset_name_or_id: \n",
    "    site-1: \n",
    "        id: \"009\"\n",
    "        name: \"Task09_Spleen-1\"\n",
    "    site-2:\n",
    "        id: \"010\"\n",
    "        name: \"Task09_Spleen-2\"\n",
    "experiment_name: \"Task09_Spleen\"\n",
    "tracking_uri: \"http://localhost:5000\"\n",
    "nnunet_trainer: \"nnUNetTrainer_10epochs\"\n",
    "num_rounds: 10\n",
    "start_round: 0\n",
    "local_epochs: 1\n",
    "server_bundle_root: \"/home/maia-user/Data/NVFlare_POC/MONet-Bundles/server/MONetBundle\"\n",
    "modality_list:\n",
    "- \"CT\"\n",
    "label_dict:\n",
    "    Spleen: 1\n",
    "bundle_extra_config:\n",
    "  is_federated: True\n",
    "#  resume_epoch: 5  # start_round * local_epochs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.nvflare.nvflare_generate_job_configs import generate_configs\n",
    "\n",
    "generate_configs(\n",
    "    [str(Path(ROOT_FOLDER).joinpath(\"Clients\",client+\".yaml\")) for client in clients],\n",
    "    str(Path(ROOT_FOLDER).joinpath(\"Experiments\",experiment+\".yaml\")),\n",
    "    str(Path(ROOT_FOLDER).joinpath(\"src\",experiment)),\n",
    "    str(Path(ROOT_FOLDER).joinpath(\"Jobs\",experiment)),\n",
    "    tasks = [\"prepare_bundle\",\"train_fl_monet_bundle\"]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb860d83",
   "metadata": {},
   "source": [
    "### Prepare Bundle for FL Training\n",
    "\n",
    "The MONet Bundle has to be prepared for the Federated Learning training. In this step, the train and evaluation bundle parameters will be overwritten to match the Federated Learning training configuration.\n",
    "\n",
    "To prepare the MONet Bundle for the Federated Learning training, run the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4285b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from monai.nvflare.nvflare_nnunet import run_job\n",
    "\n",
    "task_name = \"prepare_bundle\"\n",
    "\n",
    "job_id = run_job(sess, task_name, str(Path(JOB_DIR).joinpath(\"jobs\")),\n",
    "                 \n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a28aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.monitor_job(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831fa6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dir = sess.download_job_result(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a19777",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(job_dir).joinpath(\"workspace\",\"nnUNet_prepare_bundle\",\"bundle_config.json\"),\"r\") as f:\n",
    "    bundle_config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf35d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(bundle_config[\"site-1\"], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039eaf71",
   "metadata": {},
   "source": [
    "### Run FL Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20744dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from monai.nvflare.nvflare_nnunet import run_job\n",
    "\n",
    "task_name = \"train_fl_monet_bundle\"\n",
    "\n",
    "job_id = run_job(sess, task_name, str(Path(JOB_DIR).joinpath(\"jobs\")),\n",
    "                 \n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7253c1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.monitor_job(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c240462e-d098-49c9-aa1e-a50ef6c9d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.abort_job(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b65df78",
   "metadata": {},
   "source": [
    "To download the trained models from the clients and upload them to the server, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86caea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dir = sess.download_job_result(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f1d609",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Resume FL Training from Checkpoint\n",
    "\n",
    "To resume the Federated Learning training from an existing checkpoint:\n",
    "\n",
    "1. Download the global model from the server (``` sess.download_job_result(job_id)```) and upload them to the MONAI Bundle in the server (`<BUNDLE_ROOT>/models/fold_<id>`)\n",
    "\n",
    "2. Add the following parameters to the server configuration file:\n",
    "\n",
    "```yaml\n",
    "network_def_fl:\n",
    "  _target_: $monai.apps.nnunet.nnunet_bundle.get_network_from_nnunet_plans\n",
    "  plans_file: \"$@bundle_root+'/models/plans.json'\"\n",
    "  dataset_file: \"$@bundle_root+'/models/dataset.json'\"\n",
    "  configuration: '@nnunet_configuration'\n",
    "  model_ckpt: \"$@ckpt_dir+'/FL_global_model.pt'\"\n",
    "```\n",
    "\n",
    "3. Update the Experiment Configuration file with the new start_round and num_rounds values.\n",
    "\n",
    "```yaml\n",
    "start_round: <START_ROUND>\n",
    "num_rounds: <INITIAL_ROUNDS - START_ROUND>\n",
    "bundle_extra_config:\n",
    "  resume_epoch: 20  # start_round * local_epochs\n",
    "```\n",
    "4. Re-execute the `generate_job_configs` script to update the job configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554dba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.nvflare.nvflare_generate_job_configs import generate_configs\n",
    "\n",
    "generate_configs(\n",
    "    [str(Path(ROOT_FOLDER).joinpath(\"Clients\",client+\".yaml\")) for client in clients],\n",
    "    str(Path(ROOT_FOLDER).joinpath(\"Experiments\",experiment+\".yaml\")),\n",
    "    str(Path(ROOT_FOLDER).joinpath(\"src\",experiment)),\n",
    "    str(Path(ROOT_FOLDER).joinpath(\"Jobs\",experiment)),\n",
    "    tasks = [\"prepare_bundle\",\"train_fl_monet_bundle\"]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c602f4c",
   "metadata": {},
   "source": [
    "## Export Federated Model as MONet Bundle\n",
    "\n",
    "To export the trained Federated Learning model as a MONet Bundle, you need to download the global trained model from the server and prepare the MONet Bundle for deployment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1534e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dir = sess.download_job_result(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feae3cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FL_global_model = Path(job_dir).joinpath(\"workspace\",\"app_server\",\"FL_global_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cceb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp $FL_global_model_path \\\n",
    "   /home/maia-user/Data/NVFlare_POC/MONet-Bundles/site-1/MONetBundle/models/fold_0/FL_global_model.pt\n",
    "\n",
    "cp /home/maia-user/Data/NVFlare_POC/MONet-Bundles/server/MONetBundle/models/dataset.json \\\n",
    "    /home/maia-user/Data/NVFlare_POC/MONet-Bundles/site-1/MONetBundle/models/dataset.json\n",
    "\n",
    "cp /home/maia-user/Data/NVFlare_POC/MONet-Bundles/server/MONetBundle/models/plans.json \\\n",
    "    /home/maia-user/Data/NVFlare_POC/MONet-Bundles/site-1/MONetBundle/models/plans.json\n",
    "    \n",
    "cp $FL_global_model_path \\\n",
    "   /home/maia-user/Data/NVFlare_POC/MONet-Bundles/site-2/MONetBundle/models/fold_0/FL_global_model.pt\n",
    "\n",
    "cp /home/maia-user/Data/NVFlare_POC/MONet-Bundles/server/MONetBundle/models/dataset.json \\\n",
    "    /home/maia-user/Data/NVFlare_POC/MONet-Bundles/site-2/MONetBundle/models/dataset.json\n",
    "\n",
    "cp /home/maia-user/Data/NVFlare_POC/MONet-Bundles/server/MONetBundle/models/plans.json \\\n",
    "    /home/maia-user/Data/NVFlare_POC/MONet-Bundles/site-2/MONetBundle/models/plans.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e373c1f",
   "metadata": {},
   "source": [
    "Next, we can run the validation of the global Federated model on the individual clients. At the end of the validation, the metrics will be logged in MLFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c1e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from monai.nvflare.nvflare_nnunet import run_job\n",
    "\n",
    "task_name = \"finalize\"\n",
    "\n",
    "job_id = run_job(sess, task_name, str(Path(JOB_DIR).joinpath(\"jobs\")),\n",
    "                 \n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd5c25a",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Simpson, A. L., Antonelli, M., Bakas, S., Bilello, M., Farahani, K., van Ginneken, B., ... & Menze, B. H. (2019).  *A large annotated medical image dataset for the development and evaluation of segmentation algorithms.*  arXiv preprint [arXiv:1902.09063](https://arxiv.org/abs/1902.09063).\n",
    "\n",
    "Roth, H. R., Cheng, Y., Wen, Y., Yang, I., Xu, Z., Hsieh, Y.-T.,  Feng, A. (2022)\n",
    "*NVIDIA FLARE: Federated Learning from Simulation to Real-World.*\n",
    "doi:10.48550/arXiv.2210.13291\n",
    "\n",
    "Cardoso, M. J., Li, W., Brown, R., Ma, N., Kerfoot, E., Wang, Y.,  Feng, A. (2022). *MONAI: An open-source framework for deep learning in healthcare.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
